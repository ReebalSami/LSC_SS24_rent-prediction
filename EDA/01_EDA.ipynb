{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Exploratory Data Analysis is used to gain an understanding for the used data, to find patterns and generate insights. In the following we will analyse and clean the data, to have good dataset for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Dataset\n",
    "The first step was to understand our problem and research which data set we wanted to use. We compared these datasets: \n",
    "1.      https://www.kaggle.com/datasets/iamsouravbanerjee/...\n",
    "     ...house-rent-prediction-dataset\n",
    "2.      https://www.destatis.de/EN/Themes/Society-Environment/Housing/_node.html\n",
    "3. https://www.kaggle.com/datasets/sndorburian/house-price-index-2015-100-annual-data-in-eu\n",
    "4. https://www.kaggle.com/code/jonaslneri/german-rent-avg-by-postal-code\n",
    "\n",
    "After we compared these datasets, we came to the conclusion that the fourth dataset would be most fitting for our project.\n",
    "\n",
    "## Import Dataset\n",
    "After the selection, we imported the dataset into our project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('../data/immo_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "print(\"\"\"\n",
    "    shape:\n",
    "        {:,} rows\n",
    "        {} columns\n",
    "\"\"\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the numpy function info, we were able to see the different data types, non-null values and column descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the nature of the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nunique function calculates for every column the number of unique values. For example \"regio1\" has 16 different values in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique values in each column of the DataFrame.\n",
    "unique_counts = df.nunique()\n",
    "\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to drop\n",
    "To make the dataset better we reviewed some features and after careful condsideration dropped some features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column \"telekomHybridUploadSpeed\" has one unique value 10 for 16% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the unique values from the 'telekomHybridUploadSpeed' column in the DataFrame.\n",
    "\n",
    "unique_values = df['telekomHybridUploadSpeed'].unique()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we checked the \"street\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times the value 'no_information' appears in the 'street' column of the DataFrame.\n",
    "target_value = 'no_information'\n",
    "count = len(df[df['street'] == target_value])\n",
    "\n",
    "print(f\"The value '{target_value}' appears {count} times in the column '{'street'}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'livingSpace' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'serviceCharge']].sort_values(by='serviceCharge', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "The next step was to clean our dataset. This step is very important to ensure that the quality of the dataset is good. To do that we needed to check in the individual columns how many values are missing and make sure that the percentage of missing values isnÂ´t too high. We also needed to make sure that the values are valid and if not update the values.\n",
    "After the cleaning our dataset should be consistent and without missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df,norows):   # input by the df and the number of rows that you want to show\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = ((df.isnull().sum().sort_values(ascending=False)/df.shape[0])*100).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return(missing_data.head(norows))\n",
    "missing_values(df,49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort number of missing values for each feature, highest first.\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for the DataFrame to get a better understanding of the data.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the difference between the columns 'streetPlain' and 'street'\n",
    "selected_columns = ['streetPlain', 'street']\n",
    "print(df[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the difference between the columns 'yearConstructed' and 'yearConstructedRange'\n",
    "selected_columns = ['yearConstructed', 'yearConstructedRange']\n",
    "print(df[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the sum of baseRent and serviceCharge equals totalRent\n",
    "\n",
    "# Create a new column 'rentMatch' with default value False\n",
    "df['rentMatch'] = False\n",
    "\n",
    "# Check if the sum of baseRent and serviceCharge equals totalRent\n",
    "mask = np.isclose(df['baseRent'] + df['serviceCharge'], df['totalRent'])\n",
    "\n",
    "# Update the 'rentMatch' column where the condition is True\n",
    "df.loc[mask, 'rentMatch'] = True\n",
    "\n",
    "# Count the number of False values in the 'rentMatch' column\n",
    "false_count = (~df['rentMatch']).sum()\n",
    "\n",
    "# Print the count of False values\n",
    "print(\"Number of rows where rentMatch is False:\", false_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the sum of baseRent, serviceCharge and heatingCosts equals totalRent\n",
    "\n",
    "# Create a new column 'rentMatch' with default value False\n",
    "df['rentMatch'] = False\n",
    "\n",
    "# Check if the sum of baseRent and serviceCharge equals totalRent\n",
    "mask = np.isclose(df['baseRent'] + df['serviceCharge'] + df['heatingCosts'], df['totalRent'])\n",
    "\n",
    "# Update the 'rentMatch' column where the condition is True\n",
    "df.loc[mask, 'rentMatch'] = True\n",
    "\n",
    "# Count the number of False values in the 'rentMatch' column\n",
    "false_count = (~df['rentMatch']).sum()\n",
    "\n",
    "# Print the count of False values\n",
    "print(\"Number of rows where rentMatch is False:\", false_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all apartments with missing values in the column 'totalRent'\n",
    "df.dropna(subset=['totalRent'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'livingSpace' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'serviceCharge']].sort_values(by='serviceCharge', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of serviceCharge\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "sorted_df = df[['serviceCharge']].sort_values(by='serviceCharge', ascending=False)\n",
    "\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'serviceCharge' with 0\n",
    "df['serviceCharge'] = df['serviceCharge'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of serviceCharge\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "sorted_df = df[['serviceCharge']].sort_values(by='serviceCharge', ascending=False)\n",
    "\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of 'geo_plz'\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "sorted_df = df[['geo_plz']].sort_values(by='geo_plz', ascending=False)\n",
    "\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'geo_plz' contains values with exactly four digits\n",
    "filtered_df = df[df['geo_plz'].astype(str).str.len() == 4]\n",
    "\n",
    "# Display the 'geo_plz' and 'regio3' columns for the filtered rows\n",
    "print(filtered_df[['geo_plz', 'regio3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'baseRentRange' column\n",
    "unique_baseRent_Range = df['baseRentRange'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique values in the 'baseRentRange' column:\")\n",
    "print(unique_baseRent_Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'livingSpaceRange' column\n",
    "unique_living_Space_Range = df['livingSpaceRange'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique values in the 'livingSpaceRange' column:\")\n",
    "print(unique_living_Space_Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'noRoomsRange' column\n",
    "unique_noRooms_Range = df['noRoomsRange'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique values in the 'noRoomsRange' column:\")\n",
    "print(unique_noRooms_Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'pricetrend' column\n",
    "unique_noRooms_Range = df['pricetrend'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique values in the 'pricetrend' column:\")\n",
    "print(unique_noRooms_Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'facilities' column\n",
    "unique_facilities = df['facilities'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique values in the 'facilities' column:\")\n",
    "print(unique_facilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the 'description' column\n",
    "unique_description = df['description'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique values in the 'description' column:\")\n",
    "print(unique_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed\n",
    "columns_to_drop = [\n",
    "    'telekomHybridUploadSpeed',\n",
    "    'electricityKwhPrice',\n",
    "    'electricityBasePrice',\n",
    "    'energyEfficiencyClass',\n",
    "    'lastRefurbish',\n",
    "    'heatingCosts',\n",
    "    'noParkSpaces',\n",
    "    'petsAllowed',\n",
    "    'interiorQual',\n",
    "    'thermalChar',\n",
    "    'numberOfFloors',\n",
    "    'streetPlain',\n",
    "    'street',\n",
    "    'yearConstructedRange',\n",
    "    'rentMatch',\n",
    "    'houseNumber',\n",
    "    'telekomUploadSpeed',\n",
    "    'telekomTvOffer',\n",
    "    'regio1',\n",
    "    'regio2',\n",
    "    'scoutId',\n",
    "    'picturecount',\n",
    "    'geo_bln',\n",
    "    'geo_krs',\n",
    "    'regio3',\n",
    "    'baseRentRange',\n",
    "    'livingSpaceRange',\n",
    "    'noRoomsRange',\n",
    "    'facilities',\n",
    "    'description',\n",
    "    'baseRent',\n",
    "    'date'\n",
    "]\n",
    " \n",
    "# Delete the listed columns\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    " \n",
    "# Show updated dataframe after dropping columns\n",
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "    shape:\n",
    "        {:,} rows\n",
    "        {} columns\n",
    "\"\"\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'livingSpace' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'serviceCharge']].sort_values(by='serviceCharge', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'floor' column\n",
    "floor_counts = df['floor'].value_counts()\n",
    "\n",
    "# Display the counts of unique values\n",
    "print(floor_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value of the 'floor' column\n",
    "mean_floor = round(df['floor'].mean())\n",
    "\n",
    "# Replace NaN values and values greater than 46 with the mean value\n",
    "df['floor'] = df['floor'].fillna(mean_floor)\n",
    "df['floor'] = df['floor'].apply(lambda x: mean_floor if x > 46 else x)\n",
    "\n",
    "# Show updated dataframe\n",
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'geo_plz' column\n",
    "geo_plz_counts = df['geo_plz'].value_counts()\n",
    "\n",
    "# Display the counts of unique values\n",
    "print(geo_plz_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the 'newlyConst' column\n",
    "unique_values_newlyConst = df['newlyConst'].unique()\n",
    "\n",
    "print(unique_values_newlyConst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows where newlyConst is False and yearConstructed is NaN\n",
    "filtered_df = df[(df['newlyConst'] == False) & df['yearConstructed'].isna()]\n",
    "\n",
    "# Count the number of rows in the filtered DataFrame\n",
    "num_not_newlyConst_nan_yearConstructed = filtered_df.shape[0]\n",
    "\n",
    "print(\"Number of apartments with newlyConst as False and NaN yearConstructed:\", num_not_newlyConst_nan_yearConstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows where newlyConst is False and yearConstructed is NaN\n",
    "filtered_df = df[(df['newlyConst'] == True) & df['yearConstructed'].isna()]\n",
    "\n",
    "# Count the number of rows in the filtered DataFrame\n",
    "num_not_newlyConst_nan_yearConstructed = filtered_df.shape[0]\n",
    "\n",
    "print(\"Number of apartments with newlyConst as True and NaN yearConstructed:\", num_not_newlyConst_nan_yearConstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means all the missing values from yearConstructed are for old buildings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows where 'yearConstructed' is NaN\n",
    "nan_yearConstructed_df = df[df['yearConstructed'].isna()]\n",
    "\n",
    "# Select 'yearConstructed' and 'heatingType' columns\n",
    "nan_yearConstructed_heatingType = nan_yearConstructed_df[['yearConstructed', 'heatingType']]\n",
    "\n",
    "print(nan_yearConstructed_heatingType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where both 'yearConstructed' and 'heatingType' are NaN\n",
    "df = df.dropna(subset=['yearConstructed', 'heatingType'], how='all')\n",
    "\n",
    "# Show updated dataframe\n",
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the unique values in the 'firingTypes' and 'heatingType' columns to get a better understanding of the data\n",
    "print(df[['firingTypes', 'heatingType']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of firing types is very large. There are entries like \"gas\" aswell as entries like \"gas:oil:district_heating:electricity:coal:natural_gas_light\". Due to that we decided to only keep the most common types and combina all others in the category \"other\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the count of unique values and the most common values\n",
    "print(df['firingTypes'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping\n",
    "mapping = {\n",
    "    'gas': 'Gas',\n",
    "    'natural_gas_light': 'Gas',\n",
    "    'natural_gas_heavy': 'Gas',\n",
    "    'liquid_gas': 'Gas',\n",
    "    'gas:electricity': 'Gas',\n",
    "    'gas:district_heating': 'Gas',\n",
    "    'gas:natural_gas_heavy': 'Gas',\n",
    "    'gas:natural_gas_light': 'Gas',\n",
    "    'gas:oil': 'Gas',\n",
    "    'gas:bio_energy': 'Gas',\n",
    "    'gas:combined_heat_and_power_fossil_fuels': 'Gas',\n",
    "    'gas:heat_supply': 'Gas',\n",
    "    'gas:wood': 'Gas',\n",
    "    'gas:local_heating': 'Gas',\n",
    "    'gas:district_heating:electricity': 'Gas',\n",
    "    'gas:combined_heat_and_power_regenerative_energy': 'Gas',\n",
    "    'gas:environmental_thermal_energy': 'Gas',\n",
    "    'gas:wood_chips': 'Gas',\n",
    "    'gas:oil:district_heating:electricity:coal:natural_gas_light': 'Gas',\n",
    "    'oil': 'Oil',\n",
    "    'oil:electricity': 'Oil',\n",
    "    'oil:district_heating': 'Oil',\n",
    "    'oil:wood': 'Oil',\n",
    "    'oil:natural_gas_light': 'Oil',\n",
    "    'oil:wood:coal:natural_gas_light': 'Oil',\n",
    "    'electricity': 'Electricity',\n",
    "    'electricity:natural_gas_heavy': 'Electricity',\n",
    "    'electricity:natural_gas_light': 'Electricity',\n",
    "    'electricity:combined_heat_and_power_fossil_fuels': 'Electricity',\n",
    "    'electricity:wood': 'Electricity',\n",
    "    'electricity:local_heating': 'Electricity',\n",
    "    'electricity:coal:natural_gas_light:natural_gas_heavy:liquid_gas:steam_district_heating:wood:wood_chips:coal_coke:local_heating:heat_supply:bio_energy:wind_energy': 'Electricity',\n",
    "    'electricity:wood_chips': 'Electricity',\n",
    "    'electricity:bio_energy': 'Electricity',\n",
    "    'electricity:environmental_thermal_energy': 'Electricity',\n",
    "    'district_heating': 'District Heating',\n",
    "    'district_heating:electricity': 'District Heating',\n",
    "    'district_heating:local_heating': 'District Heating',\n",
    "    'district_heating:combined_heat_and_power_renewable_energy': 'District Heating',\n",
    "    'district_heating:combined_heat_and_power_fossil_fuels': 'District Heating',\n",
    "    'district_heating:coal': 'District Heating',\n",
    "    'district_heating:bio_energy': 'District Heating',\n",
    "    'district_heating:wood:bio_energy': 'District Heating',\n",
    "    'district_heating:heat_supply': 'District Heating',\n",
    "    'district_heating:natural_gas_heavy': 'District Heating',\n",
    "    'district_heating:natural_gas_light': 'District Heating',\n",
    "    'district_heating:hydro_energy': 'District Heating',\n",
    "    'district_heating:local_heating:combined_heat_and_power_renewable_energy': 'District Heating',\n",
    "    'district_heating:combined_heat_and_power_fossil_fuels:combined_heat_and_power_regenerative_energy': 'District Heating',\n",
    "    'combined_heat_and_power_fossil_fuels': 'Combined Heat and Power (CHP)',\n",
    "    'combined_heat_and_power_renewable_energy': 'Combined Heat and Power (CHP)',\n",
    "    'combined_heat_and_power_regenerative_energy': 'Combined Heat and Power (CHP)',\n",
    "    'combined_heat_and_power_bio_energy': 'Combined Heat and Power (CHP)',\n",
    "    'bio_energy': 'Biomass',\n",
    "    'pellet_heating': 'Biomass',\n",
    "    'wood_chips': 'Biomass',\n",
    "    'wood': 'Biomass',\n",
    "    'coal': 'Coal',\n",
    "    'coal_coke': 'Coal',\n",
    "    'geothermal': 'Geothermal',\n",
    "    'geothermal:gas': 'Geothermal',\n",
    "    'geothermal:solar_heating': 'Geothermal',\n",
    "    'geothermal:district_heating': 'Geothermal',\n",
    "    'geothermal:solar_heating:gas': 'Geothermal',\n",
    "    'geothermal:solar_heating:pellet_heating': 'Geothermal',\n",
    "    'geothermal:electricity': 'Geothermal',\n",
    "    'geothermal:combined_heat_and_power_fossil_fuels': 'Geothermal',\n",
    "    'geothermal:bio_energy': 'Geothermal',\n",
    "    'geothermal:solar_heating:pellet_heating:district_heating': 'Geothermal',\n",
    "    'solar_heating': 'Renewable Energy',\n",
    "    'solar_heating:gas': 'Renewable Energy',\n",
    "    'solar_heating:district_heating': 'Renewable Energy',\n",
    "    'solar_heating:oil': 'Renewable Energy',\n",
    "    'solar_heating:pellet_heating': 'Renewable Energy',\n",
    "    'solar_heating:environmental_thermal_energy': 'Renewable Energy',\n",
    "    'solar_heating:gas:electricity': 'Renewable Energy',\n",
    "    'solar_heating:gas:wood': 'Renewable Energy',\n",
    "    'solar_heating:gas:natural_gas_light': 'Renewable Energy',\n",
    "    'solar_heating:local_heating': 'Renewable Energy',\n",
    "    'solar_heating:bio_energy': 'Renewable Energy',\n",
    "    'solar_heating:gas:district_heating': 'Renewable Energy',\n",
    "    'solar_heating:wood:environmental_thermal_energy': 'Renewable Energy',\n",
    "    'wind_energy': 'Renewable Energy',\n",
    "    'hydro_energy': 'Renewable Energy',\n",
    "    'environmental_thermal_energy': 'Renewable Energy',\n",
    "    'gas:environmental_thermal_energy': 'Renewable Energy',\n",
    "    'electricity:environmental_thermal_energy': 'Renewable Energy',\n",
    "    'solar_heating:natural_gas_light:environmental_thermal_energy': 'Renewable Energy',\n",
    "    'gas:electricity:environmental_thermal_energy': 'Renewable Energy',\n",
    "    'liquid_gas:steam_district_heating:wood:wood_chips:coal_coke:local_heating:heat_supply:bio_energy:wind_energy:hydro_energy:environmental_thermal_energy:combined_heat_and_power_fossil_fuels': 'Others',\n",
    "    'electricity:coal:natural_gas_light:natural_gas_heavy:liquid_gas:steam_district_heating:wood:wood_chips:coal_coke:local_heating:heat_supply:bio_energy:wind_energy': 'Others',\n",
    "    'district_heating:local_heating:combined_heat_and_power_fossil_fuels': 'Others',\n",
    "    'district_heating:electricity:local_heating:combined_heat_and_power_fossil_fuels': 'Others',\n",
    "    'geothermal:solar_heating:pellet_heating:district_heating': 'Others',\n",
    "    'gas:natural_gas_light:heat_supply': 'Others',\n",
    "    'natural_gas_light:liquid_gas': 'Others',\n",
    "    'natural_gas_light:natural_gas_heavy': 'Others',\n",
    "    'natural_gas_light:wood': 'Others',\n",
    "    'natural_gas_light:heat_supply': 'Others',\n",
    "    'natural_gas_heavy:local_heating': 'Others',\n",
    "    'solar_heating:heat_supply': 'Others',\n",
    "    'district_heating:combined_heat_and_power_regenerative_energy': 'Others',\n",
    "    'local_heating': 'Others',\n",
    "    'district_heating:wood': 'Others',\n",
    "    'district_heating:local_heating:combined_heat_and_power_fossil_fuels': 'Others',\n",
    "    'district_heating:electricity:natural_gas_heavy': 'Others',\n",
    "    'district_heating:electricity:local_heating:combined_heat_and_power_fossil_fuels': 'Others',\n",
    "    'district_heating:wood:bio_energy': 'Others',\n",
    "    'district_heating:heat_supply': 'Others',\n",
    "    'geothermal:solar_heating:pellet_heating': 'Others',\n",
    "    'solar_heating:wood_chips': 'Others',\n",
    "    'solar_heating:gas:bio_energy': 'Others',\n",
    "    'pellet_heating:gas': 'Others',\n",
    "    'pellet_heating:district_heating': 'Others',\n",
    "    'pellet_heating:electricity': 'Others',\n",
    "    'pellet_heating:oil': 'Others',\n",
    "    'pellet_heating:wood': 'Others',\n",
    "    'geothermal:solar_heating:pellet_heating:gas': 'Others',\n",
    "    'geothermal:combined_heat_and_power_fossil_fuels': 'Others',\n",
    "    'solar_heating:gas:electricity:coal:coal_coke:local_heating': 'Others',\n",
    "    'solar_heating:oil:electricity': 'Others',\n",
    "    'solar_heating:pellet_heating:bio_energy': 'Others',\n",
    "    'solar_heating:gas:natural_gas_light': 'Others',\n",
    "    'solar_heating:environmental_thermal_energy': 'Others',\n",
    "    'solar_heating:electricity': 'Others',\n",
    "    'solar_heating:natural_gas_light': 'Others',\n",
    "    'gas:district_heating:local_heating': 'Others',\n",
    "    'local_heating:combined_heat_and_power_fossil_fuels': 'Others',\n",
    "    'gas:oil:electricity': 'Others',\n",
    "    'solar_heating:pellet_heating:gas': 'Others',\n",
    "    'gas:liquid_gas': 'Gas'\n",
    " \n",
    "}\n",
    " \n",
    "# Update the firingTypes column\n",
    "df['firingTypes'] = df['firingTypes'].map(mapping).fillna(df['firingTypes'])\n",
    " \n",
    "# Print the counts for each new category\n",
    "firing_types_counts = df['firingTypes'].value_counts(dropna=False)\n",
    " \n",
    "# Print the count of unique values in the 'firingTypes' column\n",
    "print(\"Number of unique values in the 'firingTypes' column:\", len(firing_types_counts))\n",
    " \n",
    "# Print a line separator\n",
    "print(\"-\" * 50)\n",
    " \n",
    "# Print the counts for each unique value in the 'firingTypes' column\n",
    "print(\"Counts for each unique value in the 'firingTypes' column:\")\n",
    "print(firing_types_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['heatingType'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heating_mapping = {\n",
    "    'central_heating': 'Central Heating',\n",
    "    'district_heating': 'District Heating',\n",
    "    'gas_heating': 'Central Heating',\n",
    "    'floor_heating': 'Electric Heating',\n",
    "    'self_contained_central_heating': 'Central Heating',\n",
    "    'oil_heating': 'Central Heating',\n",
    "    'heat_pump': 'Renewable Energy Heating',\n",
    "    'combined_heat_and_power_plant': 'Central Heating',\n",
    "    'night_storage_heater': 'Electric Heating',\n",
    "    'wood_pellet_heating': 'Biomass Heating',\n",
    "    'electric_heating': 'Electric Heating',\n",
    "    'stove_heating': 'Biomass Heating',\n",
    "    'solar_heating': 'Renewable Energy Heating'\n",
    "}\n",
    "\n",
    "# Update the 'heatingType' column based on the mapping table\n",
    "df['heatingType'] = df['heatingType'].map(heating_mapping, na_action='ignore')\n",
    "\n",
    "# Check the new distribution of 'heatingType'\n",
    "print(df['heatingType'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some outliers, beside NaN in the 'yearConstructed' column, but they are not missing values. The model can handle them as they are. The NaN values will be imputed in later step.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Sort the DataFrame by the 'yearConstructed' column in descending order\n",
    "sorted_df = df[['yearConstructed']].sort_values(by='yearConstructed', ascending=False)\n",
    "\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'heatingType' is 'central_heating'\n",
    "central_heating_df = df[df['heatingType'] == 'central_heating']\n",
    "\n",
    "# Calculate the range of years\n",
    "min_year = central_heating_df['yearConstructed'].min()\n",
    "max_year = central_heating_df['yearConstructed'].max()\n",
    "\n",
    "print(\"Approximate range of years central heating was built in Germany the most:\")\n",
    "print(\"From:\", min_year, \"to\", max_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wont help because of the Outlayers. So we need Boxplots to get better approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = df[df['yearConstructed'].isna()].groupby('heatingType').size()\n",
    "\n",
    "# Display the counts\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the impact of 'yearConstructed' on 'heatingType'\n",
    "if 'yearConstructed' in df.columns:\n",
    "    grouped_by_type = df.groupby('heatingType')['yearConstructed'].median().sort_values()\n",
    "    print(\"Median-Baujahre fÃ¼r jeden Wohnungstyp (sortiert nach aufsteigendem Baujahr):\")\n",
    "    print(grouped_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in the 'yearConstructed' column\n",
    "unique_values_yearConstructed = df['yearConstructed'].value_counts()\n",
    "\n",
    "print(\"Unique values in yearConstructed:\")\n",
    "print(unique_values_yearConstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='yearConstructed', y='heatingType', data=df, hue='heatingType', palette=\"Set3\", legend=False)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Year Constructed vs. Heating Type')\n",
    "plt.xlabel('Year Constructed')\n",
    "plt.ylabel('Heating Type')\n",
    "\n",
    "# Set x-axis limits\n",
    "plt.xlim(1900, 2020)\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the boxplot a clear dependency between construction years and heating systems can be observed. For example in the 1940s new propertys were usually built with a central heating. To maintain the dataset's integrity and adequately replace missing values, imputation based on an empirical distribution, grouped by 'heatingType', was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nans_with_empirical_distribution(df, col_to_replace, group_by_col):\n",
    "    # Get the unique heating types\n",
    "    heating_types = df[group_by_col].unique()\n",
    "    \n",
    "    for ht in heating_types:\n",
    "        # Filter the data for the current heating type\n",
    "        subset = df[df[group_by_col] == ht]\n",
    "        \n",
    "        # Get the non-NaN values for 'yearConstructed'\n",
    "        non_nan_values = subset[col_to_replace].dropna()\n",
    "        \n",
    "        if not non_nan_values.empty:\n",
    "            # Calculate the empirical cumulative distribution function (ECDF)\n",
    "            hist, bin_edges = np.histogram(non_nan_values, bins=100, density=True)\n",
    "            cdf = np.cumsum(hist) / np.sum(hist)\n",
    "            \n",
    "            # Generate random values according to the CDF\n",
    "            nan_count = subset[col_to_replace].isna().sum()\n",
    "            random_values = np.random.rand(nan_count)\n",
    "            replacement_values = np.interp(random_values, cdf, bin_edges[:-1])\n",
    "            \n",
    "            # Round replacement values to no decimals after the comma\n",
    "            replacement_values = np.round(replacement_values).astype(int)\n",
    "            \n",
    "            # Replace NaNs with the generated values\n",
    "            nan_indices = subset[subset[col_to_replace].isna()].index\n",
    "            df.loc[nan_indices, col_to_replace] = replacement_values\n",
    "\n",
    "# Replace NaNs in 'yearConstructed' column based on empirical distribution for each 'heatingType'\n",
    "fill_nans_with_empirical_distribution(df, 'yearConstructed', 'heatingType')\n",
    "\n",
    "# Show the modified DataFrame\n",
    "print(df.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in the 'yearConstructed' column\n",
    "unique_values_yearConstructed = df['yearConstructed'].value_counts()\n",
    "\n",
    "print(\"Unique values in yearConstructed:\")\n",
    "print(unique_values_yearConstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crosstabulation to see which firingTypes are typically associated with which heatingTypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in the 'condition' column\n",
    "unique_values_condition = df['condition'].value_counts(dropna=False)\n",
    "\n",
    "print(\"Unique values in condition:\")\n",
    "print(unique_values_condition)\n",
    "\n",
    "# Check for NaN values in the 'condition' column\n",
    "nan_values_condition = df['condition'].isna().sum()\n",
    "\n",
    "print(\"\\nNumber of NaN values in condition:\")\n",
    "print(nan_values_condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in the 'condition' column with 'Other'\n",
    "df['condition'] = df['condition'].fillna('Other')\n",
    "\n",
    "# Replace specific values with 'Other'\n",
    "df['condition'] = df['condition'].replace(\n",
    "    ['ripe_for_demolition', 'need_of_renovation', 'negotiable'], 'Other'\n",
    ")\n",
    "\n",
    "# Count the unique values in the 'condition' column\n",
    "unique_values_condition = df['condition'].value_counts(dropna=False)\n",
    "\n",
    "print(\"Unique values in condition:\")\n",
    "print(unique_values_condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Sort the DataFrame by the 'totalRent' column in descending order\n",
    "sorted_df = df[['totalRent']].sort_values(by='totalRent', ascending=False)\n",
    "\n",
    "print(sorted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many false entries in total rent, that are not realistic and need to be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['totalRent'] > 200) & (df['totalRent'] < 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.crosstab(df['heatingType'], df['firingTypes']))\n",
    "\n",
    "heatingType_firingTypes_df = pd.crosstab(df['heatingType'], df['firingTypes'])\n",
    "heatingType_firingTypes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['pricetrend'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection of the presence of outliers and the mean in the pricetrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Histogramm for 'pricetrend' Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['pricetrend'].hist(bins=30, alpha=0.7, label='Pricetrend-Werte', color='blue')\n",
    "\n",
    "# Mean calculation (ignoring NaN-Values)\n",
    "mean_value = df['pricetrend'].mean()\n",
    "\n",
    "# Draw mean line\n",
    "plt.axvline(mean_value, color='red', linestyle='dashed', linewidth=2, label=f'Mittelwert: {mean_value:.2f}')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Verteilung der Pricetrend-Werte mit Mittelwert')\n",
    "plt.xlabel('Pricetrend (%)')\n",
    "plt.ylabel('HÃ¤ufigkeit')\n",
    "plt.legend()\n",
    "\n",
    "# Show Diagram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distribution and frequency of the values, especially because the mean is close to the most frequent values (such as 0.00, 3.33, 3.23, etc.), the data is symmetrically distributed, and there are no extreme outliers, we will use the mean for imputing pricetrend-NaN. This helps to maintain the overall distribution of the data. Using the mean to impute pricetrend NaN values does not alter the underlying structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean price\n",
    "mean_price = df['pricetrend'].mean()\n",
    "\n",
    "# Replace all NaN values with the average price.\n",
    "df['pricetrend'] = df['pricetrend'].fillna(mean_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "    shape:\n",
    "        {:,} rows\n",
    "        {} columns\n",
    "\"\"\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "# Show the modified DataFrame\n",
    "print(df.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initially treat the NaN values in the columns firingTypes, typeOfFlat, and heatingType as 'Other_imputed'. This approach helps to preserve data diversity and avoid information loss. Since these NaN values may contain relevant information that could affect the model outcome, we need to continuously monitor the impact on model performance and refine the data cleaning process as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'firingTypes' column\n",
    "firingTypes_counts = df['firingTypes'].value_counts(dropna=False)\n",
    "\n",
    "# Display the counts of unique values\n",
    "print(firingTypes_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values with 'Other_imputed' in the 'firingTypes' column\n",
    "df['firingTypes'] = df['firingTypes'].fillna('Other_imputed')\n",
    "\n",
    "# Counting the occurrences of each unique category in 'firingTypes' after handling NaN values\n",
    "firingTypes_counts_updated = df['firingTypes'].value_counts(dropna=False)\n",
    "\n",
    "# Displaying the updated counts\n",
    "print(firingTypes_counts_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'typeOfFlat' column\n",
    "typeOfFlat_counts = df['typeOfFlat'].value_counts(dropna=False)\n",
    "\n",
    "# Display the counts of unique values\n",
    "print(typeOfFlat_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values with 'Other_imputed' in the 'typeOfFlat' column\n",
    "df['typeOfFlat'] = df['typeOfFlat'].fillna('Other_imputed')\n",
    "\n",
    "# Counting the occurrences of each unique category in 'typeOfFlat' after handling NaN values\n",
    "typeOfFlat_counts_updated = df['typeOfFlat'].value_counts(dropna=False)\n",
    "\n",
    "# Displaying the updated counts\n",
    "print(typeOfFlat_counts_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'heatingType' column\n",
    "heatingType_counts = df['heatingType'].value_counts(dropna=False)\n",
    "\n",
    "# Display the counts of unique values\n",
    "print(heatingType_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 'Other_imputed' in the 'heatingType' column\n",
    "df['heatingType'] = df['heatingType'].fillna('Other_imputed')\n",
    "\n",
    "# Count the occurrences of each unique category in 'heatingType' after handling NaN values\n",
    "heatingType_counts_updated = df['heatingType'].value_counts(dropna=False)\n",
    "\n",
    "# Display the updated counts\n",
    "print(heatingType_counts_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "    shape:\n",
    "        {:,} rows\n",
    "        {} columns\n",
    "\"\"\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "# Show the modified DataFrame\n",
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_df = df.select_dtypes(include=[float, int, bool])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Move 'totalRent' row and column to the end\n",
    "corr_matrix = corr_matrix.reindex(index=[*corr_matrix.index[corr_matrix.index != 'totalRent'], 'totalRent'])\n",
    "corr_matrix = corr_matrix.reindex(columns=[*corr_matrix.columns[corr_matrix.columns != 'totalRent'], 'totalRent'])\n",
    "\n",
    "# Create the heatmap\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, square=True, fmt='.2f', annot=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'noRooms' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'noRooms']].sort_values(by='noRooms', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are performing a data transformation process to replace outliers in the 'noRooms' column based on specific criteria related to the 'totalRent' column. This process aims to handle extreme values in 'noRooms' that are likely to be erroneous or outliers based on the corresponding 'totalRent' values.\n",
    "\n",
    "Replacing 'noRooms' Values for Low 'totalRent':\n",
    "We first identify 'noRooms' values greater than 5 and corresponding 'totalRent' values less than 700. These cases are considered outliers as they represent unusually high numbers of rooms for properties with relatively low rents. To address this, we calculate the mean of 'noRooms' for cases where 'noRooms' is less than or equal to 5 or 'totalRent' is greater than or equal to 700. We then replace the 'noRooms' values greater than 5 and corresponding 'totalRent' values less than 700 with the calculated mean.\n",
    "\n",
    "Replacing 'noRooms' Values for High 'totalRent':\n",
    "Similarly, we identify 'noRooms' values greater than 20 and corresponding 'totalRent' values less than 2500. These instances represent extreme values of 'noRooms' for properties with relatively high rents. To handle this, we calculate the mean of 'noRooms' for cases where 'noRooms' is less than or equal to 20 or 'totalRent' is greater than or equal to 2500. We then replace the 'noRooms' values greater than 20 and corresponding 'totalRent' values less than 2500 with the calculated mean.\n",
    "\n",
    "By replacing these outlier values with more representative estimates based on the surrounding data, we aim to improve the overall quality and reliability of the 'noRooms' column, ensuring that it aligns more closely with realistic property characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of 'noRooms' excluding values that meet the conditions\n",
    "mean_noRooms = df[((df['noRooms'] <= 5) | (df['totalRent'] >= 700)) | ((df['noRooms'] <= 20) | (df['totalRent'] >= 2500))]['noRooms'].mean()\n",
    "\n",
    "# Replace the 'noRooms' values that meet the conditions with the calculated mean\n",
    "df.loc[((df['noRooms'] > 5) & (df['totalRent'] < 700)) | ((df['noRooms'] > 20) & (df['totalRent'] < 2500)), 'noRooms'] = round(mean_noRooms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'noRooms' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'noRooms']].sort_values(by='noRooms', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'garden' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'garden']].sort_values(by='garden', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'livingSpace' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'livingSpace']].sort_values(by='livingSpace', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are implementing a data preprocessing step to handle outliers in the 'livingSpace' column of our dataset. This process is tailored to intelligently replace outliers in 'livingSpace' based on the corresponding 'totalRent' values, categorizing the data into distinct rent categories and adjusting the living space values accordingly.\n",
    "\n",
    "Defining Rent Categories:\n",
    "We first define three distinct categories based on the 'totalRent' column:\n",
    "\n",
    "Category 1: 'totalRent' ranging from 0 to 1000.\n",
    "Category 2: 'totalRent' ranging from 1001 to 2000.\n",
    "Category 3: 'totalRent' exceeding 2000.\n",
    "Calculating Mean Living Space:\n",
    "For each rent category, we filter the dataset to include only the data points falling within the corresponding 'totalRent' range. Then, we calculate the mean 'livingSpace' for each category.\n",
    "\n",
    "Replacing Outliers in Living Space:\n",
    "We identify outliers in the 'livingSpace' column, defined as values greater than 600 and less than 6. For each category, we replace these outlier values with the mean 'livingSpace' calculated for that category. This approach ensures that the replacement is tailored to the specific rent range, capturing the relationship between living space and rent more accurately.\n",
    "\n",
    "By replacing outliers in 'livingSpace' in an intelligent manner based on the rent category, we aim to enhance the reliability and consistency of the dataset for subsequent analysis or modeling tasks, ensuring that the living space values align more closely with realistic property characteristics within each rent category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the categories based on totalRent\n",
    "categories = [(0, 1000), (1001, 2000), (2001, float('inf'))]\n",
    "\n",
    "# Iterate over each category\n",
    "for category in categories:\n",
    "    # Filter the DataFrame based on the totalRent category\n",
    "    category_df = df[(df['totalRent'] >= category[0]) & (df['totalRent'] <= category[1])]\n",
    "    \n",
    "    # Calculate the mean livingSpace for the current category\n",
    "    mean_livingSpace = category_df['livingSpace'].mean()\n",
    "    \n",
    "    # Replace outliers in livingSpace for the current category\n",
    "    df.loc[(df['totalRent'] >= category[0]) & (df['totalRent'] <= category[1]) & \n",
    "           ((df['livingSpace'] > 600) | (df['livingSpace'] < 6)), 'livingSpace'] = mean_livingSpace\n",
    "\n",
    "# Sort the DataFrame by the 'livingSpace' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['livingSpace', 'totalRent']].sort_values(by='livingSpace', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(sorted_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_df = df.select_dtypes(include=[float, int, bool])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Move 'totalRent' row and column to the end\n",
    "corr_matrix = corr_matrix.reindex(index=[*corr_matrix.index[corr_matrix.index != 'totalRent'], 'totalRent'])\n",
    "corr_matrix = corr_matrix.reindex(columns=[*corr_matrix.columns[corr_matrix.columns != 'totalRent'], 'totalRent'])\n",
    "\n",
    "# Create the heatmap\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, square=True, fmt='.2f', annot=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'livingSpace' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'serviceCharge']].sort_values(by='serviceCharge', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check condition and adjust\n",
    "df.loc[df['serviceCharge'] > 0.5 * df['totalRent'], 'serviceCharge'] = 0.25 * df['totalRent']\n",
    "\n",
    "# Round values to whole numbers\n",
    "df['serviceCharge'] = df['serviceCharge'].round(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'livingSpace' column in descending order and include 'totalRent'\n",
    "sorted_df = df[['totalRent', 'serviceCharge']].sort_values(by='serviceCharge', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df.nunique()\n",
    "\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataframe to a pickle file. \n",
    "# Using pickle ensures, that any changes or updates made in `EDA.ipynb` are reflected in `Visualization.ipynb` without having to manually export and import files repeatedly.\n",
    "\n",
    "df.to_pickle('../data/final_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
